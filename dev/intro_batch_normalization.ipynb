{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from typing import List, Set, Dict, Tuple, Optional, Union\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)\n",
    "\n",
    "import IPython\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "* https://kevinzakka.github.io/2016/09/14/batch_normalization/\n",
    "* https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795\n",
    "* https://medium.com/@a.mirzaei69/implement-a-neural-network-from-scratch-with-python-numpy-backpropagation-e82b70caa9bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():\n",
    "    \n",
    "    \n",
    "    def __init__(self, nn_architecture: List[dict], seed= 2021):\n",
    "        \n",
    "        self._seed = seed \n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self._nn_architecture = nn_architecture\n",
    "        \n",
    "        self.n_layers = len(nn_architecture)\n",
    "        self.cache = {}\n",
    "        \n",
    "        for idx, layer in enumerate(nn_architecture):\n",
    "            \n",
    "            self.__init_layer_parameter_random(idx, layer)\n",
    "\n",
    "    # All bias are zero\n",
    "    def _init_layer_parameter_random(self, layer_k, layer):\n",
    "        \n",
    "        pass\n",
    "        \n",
    "        \n",
    "    def _init_layer_paremeter_xavier(self):\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    # only 2 activations: relu and sigmoid\n",
    "    def activation(self, z, activation_type='relu'): -> np.array:\n",
    "        pass\n",
    "    \n",
    "        if activation_type == 'relu':\n",
    "            \n",
    "            a = np.maximum(0.0,z)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            a = 1.0/(1.0 + np.exp(-z))\n",
    "        \n",
    "        return a      \n",
    "        \n",
    "    def forward_dense_layer(self, X: np.array) -> Tuple[np.array]:\n",
    "        \n",
    "        z = W.dot(X)\n",
    "        a = self.activation(z)\n",
    "        \n",
    "        return a, z\n",
    "    \n",
    "    def forward_batchnormalization_layer(self,X: np.array): -> np.array\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward_propagation(self, X: np.array) -> np.array: \n",
    "        \n",
    "        A_curr = X\n",
    "        \n",
    "        for idx, layer in enumerate(nn_architecture):\n",
    "            \n",
    "            A_prev = A_curr\n",
    "            \n",
    "            A_curr, Z_curr = self._forward_dense_layer(A_curr) \n",
    "            \n",
    "            \n",
    "        y = A_curr\n",
    "    \n",
    "        return y, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_architecture = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 1, \"activation\": \"relu\"},\n",
    " ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization  \n",
    "\n",
    "(Breaktrough in the area)\n",
    "refs:\n",
    "* https://towardsdatascience.com/understanding-batch-normalization-with-examples-in-numpy-and-tensorflow-with-interactive-code-7f59bb126642 <= very good in simple.\n",
    "* https://kevinzakka.github.io/2016/09/14/batch_normalization/\n",
    "* Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (paper)\n",
    "    * Authors: Sergey Ioffe (same author of PLDA and works at Google) n Christian Szegedy (google)\n",
    "    * https://arxiv.org/pdf/1502.03167.pdf Paper **TODO** Read the paper. It is simple and easy to understand/ It is a good gain experience in reading paper  \n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization tensprflow doc\n",
    "\n",
    "\n",
    "refs:\n",
    "* https://www.deeplearningbook.org/contents/optimization.html\n",
    "* https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/\n",
    "* https://arxiv.org/pdf/1502.03167.pdf\n",
    "* https://arxiv.org/pdf/1702.03275.pdf\n",
    "* https://www.youtube.com/watch?v=nUUqwaxLnWs\n",
    "* https://arxiv.org/pdf/1805.11604.pdf\n",
    "* https://towardsdatascience.com/understanding-batch-normalization-with-examples-in-numpy-and-tensorflow-with-interactive-code-7f59bb126642 <= very good in simple.\n",
    "* https://towardsdatascience.com/lets-code-a-neural-network-in-plain-numpy-ae7e74410795    \n",
    "* https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/\n",
    "\n",
    "**It is being asked more often in the job interview**\n",
    "\n",
    "Batch normalization is the technique to improve the performance and stability of neural networks by normalizing the inputs in every layer so that they have mean output activation of zero and standard deviation of one.\n",
    "\n",
    "\n",
    "Donâ€™t Use With Dropout:\n",
    "\n",
    "Batch normalization offers some regularization effect, reducing generalization error, perhaps no longer requiring the use of dropout for regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Input are the values of x over a batch: $B = {x_1, x_2,..., x_i,..., x_m}$\n",
    "    * where $m$ is the batch size\n",
    "* Output: $y_i = BN_{\\gamma,\\beta}(x_i)$\n",
    "* Learning parameters: $\\gamma$ and $\\beta$\n",
    "* Normalization:\n",
    "\n",
    "$\n",
    "\\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i \\\\\n",
    "\\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2 \\\\\n",
    "z_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma^2_B + \\epsilon}} \\\\\n",
    "y_i = BN_{\\gamma,\\beta}(x_i) \\equiv \\gamma z_i + \\beta\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.01322734, 4.25871895, 4.02472907, 1.67608851, 2.38439136,\n",
       "       3.27488305, 4.73704778, 0.24529551, 4.33834569, 1.60061159])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10,); mean: 2.86; std: 1.41\n"
     ]
    }
   ],
   "source": [
    "# Batch\n",
    "X = np.random.uniform(0,5.0,size=(10))\n",
    "X\n",
    "\n",
    "print(f\"shape: {X.shape}; mean: {X.mean():.2f}; std: {X.std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.59603551,  0.99330345,  0.82768749, -0.83465937, -0.33332892,\n",
       "        0.29695316,  1.33186033, -1.84736191,  1.04966255, -0.88808126])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([-0.59603551,  0.99330345,  0.82768749, -0.83465937, -0.33332892,\n",
       "        0.29695316,  1.33186033, -1.84736191,  1.04966255, -0.88808126])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10,); mean: -0.00; std: 1.00\n"
     ]
    }
   ],
   "source": [
    "gamma = 1.0\n",
    "beta = 0.0\n",
    "epsilon = 0.0\n",
    "\n",
    "# because we did not train the layer, we are passing the mean and the variance of the batch\n",
    "Y = tf.nn.batch_normalization(X,\n",
    "                    mean = X.mean(axis=0),        # batch mean\n",
    "                    variance = X.var(axis=0),     # batch var\n",
    "                    offset = beta,scale = gamma,  # batch beta and gamma See equations  \n",
    "                    variance_epsilon = epsilon)   # batch epsilon See equations\n",
    "\n",
    "Y.numpy()\n",
    "\n",
    "# comparing with numpy\n",
    "\n",
    "Z = (X - X.mean(axis=0))/np.sqrt(X.var(axis=0) + epsilon)\n",
    "Y = gamma * Z + beta\n",
    "Y\n",
    "\n",
    "# Expectd zero mean and unit variance\n",
    "print(f\"shape: {Y.shape}; mean: {Y.mean():.2f}; std: {Y.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
