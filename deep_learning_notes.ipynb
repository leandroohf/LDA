{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "refs:\n",
    "* Coursera. deeplearning.ai\n",
    "* https://blog.goodaudience.com/artificial-neural-networks-explained-436fcf36e75\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main concepts \n",
    "\n",
    "\n",
    "ANN can learning non-linear relationships \n",
    "\n",
    "<img src=\"images/non-linear_and_linear_decision_edge.png\" width=\"400\" align=\"left\"/> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "* neuron\n",
    "\n",
    "<div style=\"clear:both\">\n",
    "<img src=\"images/neuron_ANN.png\" width=\"400\" align=\"left\"/> \n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"clear:both\">\n",
    "** activation func (**allow ANN to learn no-linear relationships**) \n",
    "\n",
    "    Good activatiobn functions has non-linear shapes, easy to compute the function and the first derivative of the function\n",
    "\n",
    "*** **sigmoid function**: most common function. widely used on logiostic regression  \n",
    "**** (range: 0.0 and 1.0)  \n",
    "**** good for binnary classifiers (output layer) \n",
    "\n",
    "\n",
    "*** **softmax**: \n",
    "**** range: vector where each element is between 0.0 and 1.0. There is nclass elements in the vector output\n",
    "**** good for multi-class classificatio (output layer)\n",
    "**** emphsaize the most likely class and return probabilities\n",
    "\n",
    "*** **tanh**: hyperbolic tangent   \n",
    "**** range: -1.0 and 1.0  \n",
    "**** mean value is zero this is good in optimization problems (remember why we should normalize the input features)  \n",
    "**** good for hidden layers  \n",
    "\n",
    "*** **ReLu**: very common  \n",
    "*** range: 0  and inf   \n",
    "*** good for hidden layers  \n",
    "</div>\n",
    "\n",
    "* layers  \n",
    "\n",
    "<div style=\"clear:both\">\n",
    "<img src=\"images/layers.jpeg\" width=\"400\" align=\"left\"/> \n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"clear:both\">\n",
    "* Loss\n",
    "\n",
    "*** **cross-entropy loss** or **or log loss**: measure the performance of classifier where the outputs ranging between 0.0 and 1.0 \n",
    "\n",
    "Cross-entropy loss increases as the predicted probability diverges from the actual label\n",
    "Is the average of log likelihood over all the data\n",
    "\n",
    "* Forward Propagation: computes the output given an input. (used in train and prediction phase)\n",
    "\n",
    "* Back-propagation: computes the gradiens in order to train the model while the ANN is learning. Only used in train phase\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train recipe\n",
    "\n",
    "refs: http://karpathy.github.io/2019/04/25/recipe/\n",
    "\n",
    "1. General advices\n",
    "\n",
    "    * fast n furious approach does not work\n",
    "    * patient and pay attention in detail tends to work (correlates with success)\n",
    "    * being defensive and obsessed about visualizations works\n",
    "    * do baby steps and avoid to **introduce a lot of unverified complexity at once**\n",
    "    * Build simple to complex\n",
    "    * ReLu are good for hidden layers\n",
    "        * Positive side learn faster than logistic and tanh due to the slope\n",
    "        * The Negative side can make train stuck caused by dead neurons where the gradient becomes zero\n",
    "    \n",
    "1. Become one with data \n",
    "\n",
    "    * inspect data\n",
    "    * try to see patterns (your brain is good at it)\n",
    "    * always check for:\n",
    "        * duplicated \n",
    "        * corrupted data\n",
    "        * wrong labels (if not systemic may not hurt to much)\n",
    "        * imbalance data\n",
    "        \n",
    "1. Set up pipeline for trainning and evaluations and test it\n",
    "\n",
    "    * work with fixed seed\n",
    "    * simplify . does not add any regularization\n",
    "    * **verify loss init**: -log(1/n_classes) for classifiers\n",
    "    * **overfit one batch or small train sample dataset as little as 2**\n",
    "    * **input independent** (shuffles labels) (the DNN should not learn. See the errors in test n val dataset)\n",
    "    * visualize the input of DNN. y_hat = model(X). Vis X.  \n",
    "\n",
    "1. Overfit (reduce bias error)\n",
    "\n",
    "    * overfit\n",
    "        * focus in **train loss** should be close to zero\n",
    "        * if you try with many models  that you increased the complexity can suggest that you have a BUG\n",
    "    * do not be a hero. start with the most related paper and copy and paste their simple architecture.\n",
    "        * for images, ResNet-50 is a good start\n",
    "        * for voice, xvectors\n",
    "    * **Adam is safe with learning rate e3-4** !? but you can try different learning rate.\n",
    "    * **Add complexity only one at time**. If you have multiple features. Suggest to add one by one and unsure you get a performance boost. Or you can try smaller image and the increase the image size \n",
    "    * **do not trust learning rate decay**. He always disable learning rate decays entirely. It is a personal advice. less problematic maybe\n",
    "\n",
    "1. Regularize (reduce variance error)\n",
    "\n",
    "    * **get more data** is by the far preferred way to regularize a model. It is **the only guarantee way to improve the model.**\n",
    "    * **data augmentation**. The next best thing\n",
    "    * **pretrain**. It is really rarely hurts to use a pre-trained network even if you have enough data.\n",
    "        * xvectors\n",
    "        * ResNet-50\n",
    "    * **make smaller input dimensionality**. Remove features that can have spurious signal (Remove ciorrelated features)\n",
    "    * **make smaller model**. Personal advise\n",
    "        * He used to use FC layers after ImageNet, but these days he uses average pooling. eliminating a tons of parameters\n",
    "    * **decrease batch size**. helps with regularization\n",
    "    * **dropout**\n",
    "    * **early stopping**\n",
    "    \n",
    "1. Tune\n",
    "\n",
    "    * **random over grid search** Never use grid serach\n",
    "    * **hyper-parameter optmization**\n",
    "    \n",
    "    \n",
    "1. Squeeze the juice (It is not preference)\n",
    "\n",
    "    * leave it training. One time he forgot one model running and get SOTA (state of the art) !?\n",
    "    * ensembles\n",
    "        * TODO: read this paper about hot to use ensemble to build one simple model: https://arxiv.org/abs/1503.02531\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate diagnostics \n",
    "\n",
    "* refs:\n",
    "    \n",
    "    * https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10   (approach to detrmine best lr)\n",
    "    * https://www.dataquest.io/blog/learning-curves-machine-learning/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/sdb1/leandro/ds_pragmatic_programming\n",
      "data_frame.png\t\t\t\t pathlib_cheatsheet_p1.png\n",
      "iris_petal_sepal.png\t\t\t pivot-table-datasheet.png\n",
      "layers.jpeg\t\t\t\t refactor_notebooks.png\n",
      "neuron_ANN.png\t\t\t\t resampling.png\n",
      "non-linear_and_linear_decision_edge.png  smote.png\n",
      "notebook_vs_code.png\t\t\t split-apply-combine.png\n",
      "onehot.png\t\t\t\t tomek.png\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and variance trade-off \n",
    "\n",
    "**Train error still to much high for the application**\n",
    "\n",
    "<img src=\"images/biasvariance.png\" height=\"250\" width=\"400\">\n",
    "\n",
    "**variance error is related to gap between train and error** \n",
    "\n",
    "There is a minimum total error\n",
    "\n",
    "<img src=\"images/irr_error.png\" height=\"250\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High bias\n",
    "\n",
    "\n",
    "* left: high bias and low variance \n",
    "* right: high bias and high variance\n",
    "\n",
    "What to do:\n",
    "\n",
    "* Adding more training instances.\n",
    "* Adding more features.\n",
    "* Feature selection.\n",
    "* Hyperparameter optimization\n",
    "* train longer (deep learning)\n",
    "\n",
    "<img src=\"images/add_data.png\" height=\"400\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Low bias high variance error\n",
    "\n",
    "\n",
    "* left: low variance \n",
    "* right: high variance\n",
    "\n",
    "What to do?\n",
    "\n",
    "* Adding more training instances.  \n",
    "\n",
    "* Increase the regularization for our current learning algorithm. This should decrease the variance and increase the bias.  \n",
    "\n",
    "    * L1 or L2\n",
    "    * dropout\n",
    "\n",
    "* Reducing the numbers of features in the training data we currently use. The algorithm will still fit the training data very well, but due to the decreased number of features, it will build less complex models. This should increase the bias and decrease the variance. \n",
    "\n",
    "\n",
    "<img src=\"images/low_high_var.png\" height=\"400\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rates \n",
    "\n",
    "Learning rate controls how much we are adjusting the weights of our network with respect the gradient of the loss function. \n",
    "\n",
    "\n",
    "<img src=\"images/learning_rate.png\" height=\"400\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Too small: **Less training time, lesser money spent on GPU cloud compute. :)**\n",
    "* Too large: does not converge\n",
    "\n",
    "<img src=\"images/learning_rate2.png\" height=\"200\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there a methodology to detrmine best learning rate?\n",
    "\n",
    "In the article **Cyclical Learning Rates for Training Neural Networks\"\"** Leslie N. Smith argued that you could estimate a good learning rate by training the model initially with a very low learning rate and increasing it (either linearly or exponentially) at each iteration.\n",
    "\n",
    "\n",
    "1. change the learningrate at each minibatch (lienarlly or exponentially)\n",
    "1. plot the learning rate (log) against loss; (choose the one close to the minumum)\n",
    "\n",
    "\n",
    "**The python package fastai has function to do that** fastai is like keras for pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips n learning curve diagnostics\n",
    "\n",
    "* https://stats.stackexchange.com/questions/345990/why-does-the-loss-accuracy-fluctuate-during-the-training-keras-lstm\n",
    "\n",
    "* https://stats.stackexchange.com/questions/187335/validation-error-less-than-training-error \n",
    "\n",
    "**Summary**\n",
    "\n",
    "* val loss or error smaller than train reasons\n",
    "    * diff in train and val data distributions. \n",
    "        * Maybe train has harder case while validation has more easy cases.\n",
    "        * data wrongly labeledin train datasets\n",
    "        * dropouts (highe level) can cause that sometimes\n",
    "\n",
    "* loss oscilation reasons\n",
    "    1. batch_size is too small\n",
    "    1. large neural network and small data  (**always compare #parmeters and #samples**)\n",
    "    \n",
    "    \n",
    "* Batch size trade off (alsoe related to previous one)\n",
    "    * too large make training slow\n",
    "    * too small loss oscilation and takes more epoch to converge\n",
    "    * large batch (**small number of mini batches** per epoch) size can make the DNN not learn\n",
    "\n",
    "You can think of model evaluation in four different categories:\n",
    "\n",
    "1. Underfitting – Validation and training error high\n",
    "\n",
    "1. Overfitting – Validation error is high, training error low\n",
    "\n",
    "1. Good fit – Validation error low, slightly higher than the training error\n",
    "\n",
    "1. Unknown fit - Validation error low, training error 'high'\n",
    "\n",
    "\n",
    "I say 'unknown' fit because the result is counter intuitive to how machine learning works. The essence of ML is to predict the unknown. If you are better at predicting the unknown than what you have 'learned', AFAIK the data between training and validation must be different in some way. \n",
    "\n",
    "\n",
    "=================================\n",
    "\n",
    "There are several reasons that can cause fluctuations in training loss over epochs. The main one though is the fact that almost all neural nets are trained with different forms of stochastic gradient decent. This is why batch_size parameter exists which determines how many samples you want to use to make one update to the model parameters. If you use all the samples for each update, you should see it decreasing and finally reaching a limit. Note that there are other reasons for the loss having some stochastic behavior.\n",
    "\n",
    "This explains why we see oscillations. But in your case, it is more that normal I would say. Looking at your code, I see two possible sources.\n",
    "\n",
    "Large network, small dataset: It seems you are training a relatively large network with 200K+ parameters with a very small number of samples, ~100. To put this into perspective, you want to learn 200K parameters or find a good local minimum in a 200K-D space using only 100 samples. Thus, you might end up just wandering around rather than locking down on a good local minima. (The wandering is also due to the second reason below).\n",
    "\n",
    "Very small batch_size. You use very small batch_size. So it's like you are trusting every small portion of the data points. Let's say within your data points, you have a mislabeled sample. This sample when combined with 2-3 even properly labeled samples, can result in an update which does not decrease the global loss, but increase it, or throw it away from a local minima. When the batch_size is larger, such effects would be reduced. Along with other reasons, it's good to have batch_size higher than some minimum. Having it too large would also make training go slow. Therefore, batch_size is treated as a hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent algorithms\n",
    "\n",
    "refs: https://stats.stackexchange.com/questions/49528/batch-gradient-descent-versus-stochastic-gradient-descent\n",
    "\n",
    "$\n",
    "J(\\theta)=\\frac{1}{2}\\sum_{i=1}^N(y_i−h_{\\theta}(x_i)^2\n",
    "$\n",
    "\n",
    "$\n",
    "\\theta_j = \\theta_j − \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j} \n",
    "$\n",
    "\n",
    "The update is given by \n",
    "\n",
    "\n",
    "$\n",
    "\\Delta \\theta_j = \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j} \\equiv  \\sum_{i=1}^N(y_i−h_{\\theta}(x_i))x_i\n",
    "$\n",
    "\n",
    "1. **Gradient descent**  \n",
    "\n",
    "    * Compute the gradient of the cost function using the entire dataset \n",
    "    * Update the weights.\n",
    "    \n",
    "    Pros n cons  \n",
    "    \n",
    "    * **Computational slow and utilizes a lot of memory**  \n",
    "    * Guarantee that loss func always will reduce   \n",
    "    \n",
    "\n",
    "1. **Stochastic Gradient Descent**\n",
    "    \n",
    "    * Compute gradient for each sample\n",
    "    \n",
    "    Pros n cons  \n",
    "    \n",
    "    * More sensible to noisy  \n",
    "    * Faster than Gradient decsent  \n",
    "    * Use less memmory   \n",
    "\n",
    "\n",
    "1. **Mini batch Gradient**  \n",
    "\n",
    "    * Compute gradient for each mini batch (This is a estimation of the true Gradient )  \n",
    "\n",
    "    Pros n Cons  \n",
    "    \n",
    "    * More robust to noisys data\n",
    "    * Faster than all methods\n",
    "    * Use less memory than Gradient but more than Stochastic\n",
    "\n",
    "\n",
    "See this discusison for batches sizes:\n",
    "* https://stats.stackexchange.com/questions/316464/how-does-batch-size-affect-convergence-of-sgd-and-why\n",
    "\n",
    ">  the minibatch size gets larger the convergence of SGD actually gets harder/worse,\n",
    "\n",
    "* Paper: https://research.fb.com/publications/accurate-large-minibatch-sgd-training-imagenet-in-1-hour/\n",
    "\n",
    ">  large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization.  \n",
    "\n",
    "* https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network/236393#236393 \n",
    "\n",
    "> It has been observed in practice that when using a larger batch there is a significant degradation in the quality of the model, as measured by its ability to generalize. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
