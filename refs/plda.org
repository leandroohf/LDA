
#+INTERLEAVE_PDF: Ioffe2006PLDA.pdf

   File better view with interleave mode (emacs tool)
   https://github.com/rudolfochrist/interleave
   
* PLDA Sergey Ioffe 2006
  :PROPERTIES:
  :INTERLEAVE_PDF: 
  :END:
  
  The article is math heavy because he find an analytic solution. So
  at least is easy to implement.
  
  https://ravisoji.com/assets/papers/Ioffe2006PLDA.pdf

** Big picture
   
   Supervised learning algorithm

   Main motivations. 
   * PLDA automatic set set dimension with less importance with low
     probabilities
   * Can be used to do classification with unseen classes (Train with
     class apple, orange n grape and latter on you decide to add the
     class rapsberry)
   * Like LDA, tranform the features in a new space with more
     discriminative power

  Passos para fazer um reconhescmento com multiplos exemplos da mesma
  classe:
    
  1. Precisamos transformar o input x no subspace the u,v
     1. Calcular \( A = W^{-t} \left ( \frac{n}{n-1} \Lambda_w \right)^{\frac{1}{2}} \)
     2. transformar o new data: \( u = A^{-1} (x - m)\)
     3. average tranformed samples of the target class: \(\bar{u^g} = \frac{1}{n} \sum_i^{n} u_k^g\)
  2. Calcular a probabilidade \( p(u,\bar{u^g}) \)
     1.  \( \Psi = max(0, \frac{n-1}{n} (\Lambda_b/\Lambda_w) - \frac{1}{n})\)
     2. \( P(u \mid \bar{u^g})= N \left (u \mid \frac{n \Psi}{ n \Psi+I} \bar{u^g},I+\frac{\Psi}}{n \Psi + I} \right) \)


  Assumptions of PLDA:

  1. All classes are normal distributed: $P(x \mid y) = N(x \mid y,\Phi_w)$
     where y is label
  2. All class has the same covariance matrix \( \Phi_w \)
  3. For Analytic solution
     * n_k = n const: Each class has the same number of samples. Case
       it is not true, resample the class in order to have all class
       with the same number of observations
  4. For EM solution does not have previous restriction

  The solution has the constraints:
     1. $\Phi_w$ is positive defined
     2. The priori $P(y) = N(y \mid 0,\Phi_b)$ is normal distributed
     3. $\Phi_b$ is semi-positive defined
        
  There are some papers that proposed PLDA whithout the homodascity assumption:
  https://www.superlectures.com/odyssey2014/exploring-some-limits-of-gaussian-plda-modeling-for-i-vector-distributions
  https://www.cse.ust.hk/~yuzhangcse/papers/Zhang_Yeung_ECML09a.pdf
  
  UMA COISA LEGAL QUE VI NO PAPER eh medir a performance do algoritmo
  qdo o numero de claases aumenta (se eu tuver 100 usuarios e se eu
  tiver 100k usuario)

  LDA que ele se referece nao eh FIsher LDA , nao eh LDA que eu
  estudei. Eh o LDA do artigo: 

  Hastie, T., Tibshirani, R.: Discriminant analysis by Gaussian mixtures. Journal of the Royal
  Statistical Society series B 58 (1996) 158â€“176
  
  Que eh uma LDA (Nao eh a de Fisher), Onde no lugar de assumir uma
  Gaussiana para cada classe, eh assumido uma mistura de Gaussianas 

  Do Abstract

  " LDA eh equivalente classificador da razao da maxima
  verossimelhanca assumindo uma Gaussiana para cada classes. Neste
  artigo ele ajusta uma istura de Gaussianas para cada classe. E isto
  facilita a construcao de classificdores mais efetivos para classes
  que seu dados nao sao normal ditribuidos, especificamnete qdo as
  classes sao agrupadas (clusterized)"


  =================================
  https://blog.csdn.net/JackyTintin/article/details/79803501
  
  Rascunho formulas desta pagina q pode me ajudar. Nela tem PLDA via EM algoritmo

  Mas nao sei se batem com a solucao exata analitica. Preciso
  confirmar se fazem sentido. No meu entendimento deve fazer

** Summary of equations:
  
  Remember: $n = N/K$ where K is the number of class.
  
  1. Fisher LDA part
     1. \(m = \frac{1}{N}\sum_i^N x_i\)
     2. \( n_k = n  \equiv N\k  \) onde K eh o numero de classes
     3. \( S_w = \frac{1}{N} \sum_k \sum_{i \in C_k} (x_i - m_k)(x_i-m_k)^t \) within class spread
     4. \( S_b = \frac{1}{N} \sum_k n_k (m_k - m)(m_k - m)^t \)  between class spread
     5. \( S_b w = \lambda S_w w \) Generalized generalized eigenvalue
        problem. But in this case we add the restriction that *Sw is
        positive defined*
     6. $S_t = \sum_{j=1}^{N}(x_j - m)(x_j - m)^t$ is the total scatter matrix 
  2. Generative model formulas
     1. \( x = m + Au \): $x$ the input data. $u$ represents an example of that class in
        the projected space and \( u \thicksim N(u \mid  v , I) \)
     3. \( y = m + Av \): $y$ are the labels (assumed to be continuos)
        $v$ represents the class in the projected space and \(
        v \thicksim N(v \mid  0, \Psi ) \)
  3. Learning PLDA parameters
     1. \( \Phi_w = \frac{n}{n-1} S_w\ \) IS TRUE (checked)
     2. \( \Phi_b = S_b - \frac{1}{n-1} S_w \) IS  TRUE  checked
     4. \( W S_t W^t  =  W S_w W^t +  W S_b W^t=  \Lambda_w + \Lambda_b = \Lambda_t\)  Conservacao do espalhamento dos dados.   It is True I checked
     5. $W$ solve the generalized eigenvector eigenvalue problem $S_bW = \lambda S_w W$. So $S_w^{-1}S_b w = \lambda w$ and $\tilde{x} = W^t x$  
        see pdf: [[file:Elhabian_LDA09.pdf]] and see [[file:../intro_fishers_lda.ipynb]]
     6. \( \Phi_b = A \Psi A^t \) this is true
     7. \( \Phi_w = A A^t \) this is true
  4. Predict with Probabilistic LDA
     1. \( \Lambda_b = W^tS_b W \) (diagnolized between class spread or Just the transfored version of $S_b$)
     2. \( \Lambda_w = W^tS_w W \) (diagnolized within class spread or Just the transfored version of $S_w$)
     3. \( A  = W^{-t} \left ( \frac{n}{n-1} \Lambda_w \right)^{\frac{1}{2}} \)
     4. \( u = A^{-1} (x - m)\)
     5. \( \Psi = max(0, \frac{n-1}{n} (\Lambda_b/\Lambda_w) - \frac{1}{n}) \)
  
  Data space $(x,y)$ and projected/transformed space $(u,v)$: 
  * \( (x_1,x_2) \to^A (u_1,u_2)  \)
  * \( (x_1,x_2) \to^W (u_1,u_2) \) It is true. only the direction
    might the oposite but if u is eigenvector, also -u so that is fine, I CHECKED
  * \( y \to^A v \)
  * \( P(x \mid y) =  N(x \mid y,\Phi_w) \to^A P(u \mid v) = N(u \mid v,I) \) Why?: \(  x = m + A u  \)
  * \( P(y) =  N(y \mid m,\Phi_b) \to^A P(v) = N(v \mid 0,\Psi) \) Why?: \(  y = m + A v  \)
  
** Using PLDA (doing predictions)

   * Task classification: 

     We know the new (probe) examples $x^p$ is one of the M classes
     used in training $(x^1, ..., x^M)$

     1. For each class in training data ($g = 1, ..., M$) (Ex: a speaker in ASR), compute the likelihood of the new data and the class $g$:
        
        \( P(u \mid \bar{u^g})= N \left (u \mid \frac{n \Psi}{ n \Psi+I} \bar{u^g},I+\frac{\Psi}}{n \Psi + I} \right) \)
        
        \bar{u^g} is the mean of all sample of the training class $g$. (mean of all utterance of a speaker). This improve performance. Better than use one example

     2. Return the class $g$ with the maximum likelihood value
   
   * Task Hypothesis testing: NEED to undertand better the Eq
     
     I think this one is used in the ivector paper n xvector paper about ASR
     
     Given to unseen classes (Ex: 2 speakers that was not in the
     training phase), we need to determine wheter they belong to the
     same class.

     \(R(\{u^p_{1, ..., m}\},\{u^g_{1, ..., n}\}) = \frac{likelihood(same)}{likelihood(diff)} \) 
     \(R(\{u^p_{1, ..., m}\},\{u^g_{1, ..., n}\}) = \frac{P(u^p_{1, ..., m},u^g_{1, ..., n})}{P(u^p_{1, ..., m})P(u^p_{1, ..., m})} \) 

* Notes for page 1
  :PROPERTIES:
  :interleave_page_note: 1
  :INTERLEAVE_PDF: Ioffe2006PLDA.pdf
  :END:

  Motivations

  refs: https://ravisoji.com/assets/papers/Ioffe2006PLDA.pdf (abstract)

  1. Fisher LDA is common used in object recognition for feature
     extraction, but do not address the problem of how to use these
     features for recognition.
  2. latent variables of PLDA (PLDA components) represent both: the
     class of the object and the within variability variability class
     of the object.
  3. Automatic give more wieght of the features with the most
     discriminativy power
  4. Can build a model of unseen class with only one example or can
     combine multiple examples for a better representation of the
     class
     
   Application:

   * Speaker recognition
   * Face recogintion

   We show applications to classification, hypothesis testing, class
   inference, and clustering, on classes not observed during
   training.

* Notes for page 2
  :PROPERTIES:
  :interleave_page_note: 2
  :END:

  The $S_b$ and $S_w$ is not the exact the same in Fishers LDA paper 
   * $S_w$ the differnece is the normalization constant 1/N
   * $S_b$ is the normalization constant 1/N and the term $n_k$

  
  LDA that he mentioned is Fisher's LDA. Can be used to discover the
  subsapce that maximizes the separability of the class. Maximize the
  ratio between variability class over the within variability class

* Notes for page 3
  :PROPERTIES:
  :interleave_page_note: 3
  :INTERLEAVE_PDF: Ioffe2006PLDA.pdf
  :END:

  Ainda estou entendendo:

  the latent variable y: center of a mixture component and represent the
  class. Member of the same class share the same y.

  \(P(y) = \pi_k \delta(y - \mu_k)\) is the probability mass for each
  poitn. eu acho q eh um delta de kronecker

  m = global mean (definido na secao LDA)
  m_k = class mean

  \(\Phi_w\): common of all classes covariance matrix

  \(\Phi_b\): between class covariance matrix

  $y$ represent the class center and examples of the classes are drawn
  form this distribution 

* Notes for page 5
  :PROPERTIES:
  :interleave_page_note: 5
  :INTERLEAVE_PDF: Ioffe2006PLDA.pdf
  :END:

  The between-class feature variance \(\Phi_t\) indicate how
  discriminative the features are.

* Notes for page 6
  :PROPERTIES:
  :interleave_page_note: 6
  :INTERLEAVE_PDF: Ioffe2006PLDA.pdf
  :END:

  Oq sao os parametros $\Lambda_w$ e $\Lambda_b$ e qual
  a relacao com $\Phi_w$ e $\Phi_b$? 

  sao as matrizes de espalahemnto trasnformadas
  

  Parameters to be learned:
  
  1. \(m \) : global mean !?
  2. \( \Psi \): the covariance matrix
  3. \( A \): The loading matrix or the equilvalent $\Phi_w$ and $\Phi_b$

  The log likelihood of the data: N trainning patterns separated by K classes ($n = N/K$):

  \( l(X={x^1....x^N}) = \sum_{k=1}^K ln P(x^i: i \in C_k)\) Nao confundir o N aqui com n

  where:

  \( P(x^1, x^2, ..., x^n) = \int N(y \mid 0,\Phi_b)N(x^1 \mid y,\Phi_w)...N(x^n \mid y,\Phi_w) dy\) aqui eh n minusculo mesmo

  You can solve the integral:

  \( l(X= {x^1...x^N}) =  -\frac{c}{2} \left ( ln \mid \Phi_b + \frac{1}{n}\Phi_w \mid ) + tr((\Phi_b+\frac{1}{n}\Pgi_w)^{-1}S_b) + \)
  \( + (n-1) ln \mid \Phi_w \mid  + n tr(\Phi_w^{-1}S_w) \)

  We need maximize $l(X= {x^1...x^N})$ with respect to $\Phi_w$ and $\Phi_b$: 
  1. $\Phi_w$ being positive definite
  2. $\Phi_b$ being positive semi-definte 

  Oq eh uma matrix ser poisitive definite? 
 
  https://en.wikipedia.org/wiki/Positive-definite_matrix

  $M$ is said positive defnite if $zMz^T$ is positive scalar for no
  zeros columns in $z$. $z$ is a vector fo rela number

  Aqui eh mais restritivo pois nao pode ter ZEROS

  Oq eh uma matrix ser poisitive semi-definite? 
  
  $M$ is said positive defnite if $zMz^T$ is positive or ZERO scalar for no
  zeros columns in $z$. $z$ is a vector fo real number
  
  Whithout the 2 constraint above, simple calculation would result:

  When I read the article I understood that is not true anymore. But I
  double check it looks like is really true

  REMEMBER, this equations \( \Phi_w = \frac{n}{n-1} S_w \), \(\Phi_b
  = S_b - \frac{1}{n-1} S_w \) are +not+ true anymore (Checked. It is
  true)

  1. $W$ solve the generalized eigenvector eigenvalue problem $S_bW = \lambda S_w W$. So $S_w^{-1}S_b w = \lambda w$ and $y = W^t x$  
        see pdf: [[file:Elhabian_LDA09.pdf]] and see [[file:../intro_fishers_lda.ipynb]]
  2. \( \Phi_b = A \Psi A^t \)
  3. \( \Phi_w = A A^t \)


  =================================
  Train model receipt
  
  \(n = N/K\) : K classes
  Find parameters ( $m$, $A$ and $\Psi$ )that maximize the likelihood of PLDA: !? NAO sei qual eh a formula
  
  The paper found a analytic solution to the problem, so there is no
  iterative algorithm. You just need to compute the parameters
  
  1. Fishers LDA Steps
     1. Compute $S_w$ and $S_b$
     2. Compute W by solving the eigenvalue eigenvector problem: \(S_w^{-1}S_b w = \lambda w\)
  2. Transform the scatter matrix
     1. Comput: \( \Lambda_w \) and \( \Lambda_b \)
  3. Compute the parameters
     1. \( A = f(W,\Lambda_w) \)
     2. \( \Psi = f(\Lambda_b, \Lambda_w) \)
  4. Reduce dimensionality
     1. keep $d'$  ($d' < d$)largest elements of $\Psi$ and set the rest to zero
     2. $u = A^{-1}(x - m)$ use only the features corresponding to non-zero entries of $\Psi$


  About the data transformation: I believe the second equ is true
  because of the highlighs.
  1. \( (x_1,x_2) \to^A (u_1,u_2)  \)
  2. \( (x_1,x_2) \to^W (u_1,u_2)  \) It is true

* Notes for page 9
  :PROPERTIES:
  :interleave_page_note: 9
  :INTERLEAVE_PDF: Ioffe2006PLDA.pdf
  :END:

  Discussao da performance do algoritmo vs o numero de classes
  (individuos) na base de dados.

  Oq acontece se ao invez de tiver 100 speaker eu tiver 100k speakers?

  Eh mais idendificar speaker numa base de 100 do que de 100k, certo?

