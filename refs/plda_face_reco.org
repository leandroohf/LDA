
#+INTERLEAVE_PDF: Probabilistic_Linear_Discriminant_Analys.pdf

* Notes aout PLDA n face recognition 
  


  
* Notes for page 2
  :PROPERTIES:
  :interleave_page_note: 2
  :END:

  Maybe why the python code is generating A that way in the unit test
  is related to coliumns of F that are the eigenvectors of between
  covariance


  \( x_{ij} = \mu + F h_i + G w_{ij} + \epsilon_{ij} \)

  1. $x_{ij}$ jth image of the ith speaker 
  2. \( \mu + F h_i \) is the signal component where we have
     information about the indenty of individual and the avergae
     indicidual
  3. F: the colums of F contain the bases for between individual
     subsapce
  4. G: the colums of G contain the bases for within individual
     subsapce
  5. $h_i$ represent the identity of the speaker. If 2 or more faces
     belong to the same person, they must have the same $h_i$
  6. $w_{ij}$ 
  7. $\epsilon_{ij}$: Remained unexplained data variation 
  
  \( \epsilon_{ij} \tilde N(e \mid 0, \Sigma ) \) where $\Sigma$ is
  diainal matrix
  
  * Training
    * Use a set of $x_{ij}$. So set of speakers, eacho one with more
      than one images with pose n illumiation variations
    * \( \theta = {\mu, F, G, \Sigma} \) reains const during the
      recognition phase
  * recognition phase

    2 faces images were generated by the same $h_i$

    \( P(x \mid h_i, w_{ij, \theta} ) = N(x \mid \mu + F h_i + G w_{ij}, \Sigma) \)

    \(P(h_i) = N(h \mid 0,I) \)

    \(P(w_{ij}) = N(w \mid 0,I) \)
    

* Notes for page 3
  :PROPERTIES:
  :interleave_page_note: 3
  :END:


  * $M_1$ I preffer the Hypohesis H_1
    * $x_1$ n $x_p$ was genetate with $h_1$ AND
    * $x_2$ was genrated with $h_2$
  * $M_2$ or alternative Hypohesis H_2
    * $x_1$ was genrated with $h_1$ AND
    * $x_2$ n $x_p$ was genetate with $h_2$
      
  computing the first line of eq (10) of the paper:

  \(x_1 = \mu + F h + G w_1 + 0 w_2 + ... + 0 w_N  + \epsilon_1_ \)
  
  eq (10) can be re-written as:

  \( x' = \mu' + A y + \epsilon' \) That is the original PLDA except
  by the noisy term $\epsilon'$


  * \(P(x' \mid y) = N(x' \mid A y, \Sigma'))\)
  * \(P(y) = N(y \mid 0, I))\)
    

  The PLDA likelihood is:

  \( P(x_1, x_2, ..., x_N)) = P(X') = N(X' \mid \mu' + Ay, \Sigma') \) 
